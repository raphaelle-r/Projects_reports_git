{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9ad02c-1904-45d2-a1c5-f28362980e07",
   "metadata": {},
   "source": [
    "## LOAD PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b76e6d-8c91-45e8-81c2-aeae6fa9ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import Image  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import plotly.express as px\n",
    "import time\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import os\n",
    "import statistics\n",
    "from statistics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e453209-050c-43b5-a083-073b11c65b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import neighbors\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6867573-90ca-40fb-8831-10275d821cd2",
   "metadata": {},
   "source": [
    "## LOAD FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167730f9-619a-4c62-80e7-33f2a6676799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da405764-52aa-4d76-ae00-a8d101c5bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing one hot encoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class OneHotEncoder:\n",
    "    \n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "         OneHotEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = OneHotEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = OneHotEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0035ed-303a-42cf-8e1a-b25485386cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npa\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "#detetction only on numeric data so need to encode if we want on all the data\n",
    "\n",
    "def pre_detect_outlier(df, threshold):\n",
    "\n",
    "    outlier_i = []\n",
    "    outlier_j=[]\n",
    "    outlier_var=[]\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns #'int','float', 'cfloat'\n",
    "    #print(numeric_cols)\n",
    "    if len(numeric_cols)>0: \n",
    "        data_zscore =df[numeric_cols].apply(stats.zscore)\n",
    "        data_zscore = data_zscore.abs()\n",
    "\n",
    "            #outliers preprocessing numeric\n",
    "        for var in data_zscore.columns: #name of the var, here only numeric\n",
    "            var_out=0\n",
    "            for i in range(data_zscore.shape[0]):\n",
    "                #print(i)\n",
    "                if data_zscore.loc[i,var] > threshold:\n",
    "                    var_out=var_out+1\n",
    "                    outlier_i.append(i)\n",
    "                    outlier_j.append(list(data_zscore.columns).index(var))\n",
    "                    if var_out> data_zscore.shape[0]/2:\n",
    "                        outlier_var.append(var)\n",
    "\n",
    "\n",
    "    data_cat = df.select_dtypes(include=['object']).copy() #, \"category\"\n",
    "\n",
    "\n",
    "    #outliers preprocessing categoric\n",
    "    for var in data_cat.columns: #name of the var, here only cat\n",
    "        #print(var)\n",
    "        var_out=0\n",
    "        #pour les modèles, moins que 100\n",
    "        dict_var= pd.DataFrame.from_dict(data_cat[var].value_counts()).reset_index()\n",
    "        dict_var=dict_var.rename(columns={'attribut': var, var:'count'})\n",
    "        dict_var['count']=dict_var['count']/df.shape[0]\n",
    "        #print(dict_var)\n",
    "        torename=list(dict_var[(dict_var['count']!=1/df.shape[0]) & (dict_var['count']==min(dict_var['count']))]['index'])\n",
    "        #print(torename)\n",
    "        df[var].replace(torename, dict_var['index'][0])\n",
    "\n",
    "\n",
    "    return(outlier_i, outlier_j, outlier_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12aefd8-1933-441e-bc07-cea33c18fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df,outlier_index,outlier_var):\n",
    "    if outlier_var:\n",
    "        df=df.drop(outlier_var,inplace=True, axis=1) #columns\n",
    "    if outlier_index:\n",
    "        df=df.drop(outlier_index, axis=0) #rows\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c7bd0-2bf6-4e2a-a55d-008ce3b259fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df): #simple data cleaning\n",
    "    df= df.dropna(axis = 1, how = ‘any’) #If any NA values are present, drop that row or column.   \n",
    "    df = df.loc[:,df.apply(pd.Series.nunique) != 1] #drop cst columns\n",
    "    df=df.dropna(axis=0, how=‘any’) #same for row\n",
    "    #df=df.dropna(axis=1, how=‘all’) #if all values are NA drop that column\n",
    "    #df=df.dropna(axis=0, how=‘all’) #same for row\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a023a-44ce-40c8-9079-d4db85bcfd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation of random outliers with differents rate\n",
    "\n",
    "def generate_outliers_multi(df_or): \n",
    "    \n",
    "    from random import choices\n",
    "    from random import choice\n",
    "    from random import uniform\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    #generation of random outliers with differents rate\n",
    "\n",
    "\n",
    "    epsilon_ratio=[0.01,0.03,0.1,0.15] #to be sure to have at least 1 outlier generated\n",
    "    epsilon=choice(epsilon_ratio)\n",
    "    index_out=[]\n",
    "    var_out=[]\n",
    "\n",
    "    #initialisation \n",
    "    df_or=df_or.reset_index()\n",
    "    outlier_size=int(abs(epsilon*df_or.shape[0]))\n",
    "    if outlier_size==0:\n",
    "        outlier_size=1\n",
    "    data_outliers=pd.DataFrame()\n",
    "    df_or_index=[1] * df_or.shape[0] #1 for normal -1 for outlier\n",
    "    liste_string=['blanc', 'noir', 'pourpre', 'rouge', 'orange', 'jaune', 'vert', 'bleu', 'violet', 'ivoire', 'crème', 'beige', 'rose', 'kaki', 'brun', 'marron', 'bordeaux']\n",
    "    matrix_outliers=np.zeros([df_or.shape[0],df_or.shape[1]])\n",
    "\n",
    "    for i_out in range(outlier_size):\n",
    "        #print(i_out)\n",
    "        #le nombre de outliers selon ratio\n",
    "\n",
    "        outliers= df_or.sample(n = 1) #take only one of the row\n",
    "        #print(outliers)\n",
    "        index_out=outliers.index.tolist()\n",
    "        #df_or=df_or.drop(index_out) #removing the element selected, not removing but transformed\n",
    "        #print(index_out)\n",
    "        #print(df_or_index)\n",
    "        df_or_index[index_out[0]]=-1  #outlier label changed\n",
    "        #print(df_or_index)\n",
    "\n",
    "\n",
    "        for var_j in range(outliers.shape[1]): #var_j is the index\n",
    "            \n",
    "\n",
    "            if choices(['do','do_not'], weights=[0.6, 0.4])=='do': #bruit pour faire outliers on some variables or not\n",
    "\n",
    "                #si il y a un changement on le stocke dans la matrix\n",
    "\n",
    "                matrix_outliers[index_out[0]][var_j]=1\n",
    "\n",
    "                         #separate the outliers depending of the type of object directly\n",
    "                if type(outliers.iloc[0,var_j])==str: #si on a affaire à du catégorique\n",
    "\n",
    "                    df_or.iloc[index_out[0],var_j]= choice(liste_string) #generate random from a string liste\n",
    "\n",
    "                else: #si c'est du numérique\n",
    "                    cst=uniform(2,np.max(df_or[df_or.columns[var_j]])) #generate random constant\n",
    "\n",
    "                    noise=np.random.normal(0,1,1) #mean, std and nbr of elmts\n",
    "                    df_or.iloc[index_out[0],var_j]=outliers.iloc[0,var_j]*cst + noise\n",
    "\n",
    "        #data_outliers=pd.concat([data_outliers,outliers]) #to have all the outlier_size outliers\n",
    "\n",
    "\n",
    "\n",
    "            #outliers_index=['outliers'] * outlier_size\n",
    "    #outliers_df = pd.DataFrame(data=data_outliers,columns = df_or.columns)\n",
    "\n",
    "    #df = pd.concat([df_or, outliers_df]) #to have the dataset + the generated outliers\n",
    "\n",
    "        #print(len(df_or_index))\n",
    "        #print(df.shape)\n",
    "    df=df_or\n",
    "    df['label_outlier']=df_or_index\n",
    "\n",
    "    df.reset_index()\n",
    "    df,matrix_outliers=shuffle(df,matrix_outliers) #shuffle the data and the matrix the same way\n",
    "\n",
    "\n",
    "\n",
    "    return(df,matrix_outliers, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc35302-cb49-4aeb-b5a6-dbc84af9eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_for_matrix(matrix_out, algo_matrix): \n",
    "    #the matrix need to be of the same shape\n",
    "\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "\n",
    "    matrix_out=np.matrix(matrix_out)\n",
    "    algo_matrix=np.matrix(algo_matrix)\n",
    "\n",
    "    for var in range(matrix_out.shape[1]):\n",
    "        for elmt in range(matrix_out.shape[0]):\n",
    "            if (matrix_out[elmt, var]==0) and (algo_matrix[elmt, var]==0): #correct\n",
    "                TP=TP+1\n",
    "            if (matrix_out[elmt,var]==1) and (algo_matrix[elmt, var]==1): #correct\n",
    "                TN=TN+1\n",
    "            if (matrix_out[elmt, var]==1) and (algo_matrix[elmt, var]==0): #no correct\n",
    "                FP=FP+1\n",
    "            if (matrix_out[elmt, var]==0) and (algo_matrix[elmt, var]==1): #no correct\n",
    "                FN=FN+1\n",
    "\n",
    "    #confusion_matrix=np.matrix([[TP, FP], [FN, TN]])\n",
    "    ACC= (TP+TN)/ (TP+TN+FP+FN)\n",
    "    #Recall=TP/(TP+FN)\n",
    "    #Precision=TP/(TP+FP)\n",
    "    F1=2*TP/(2*TP+FP+FN)\n",
    "    \n",
    "    return TP, FN, FP,TN, ACC, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea42ce-a8c3-402c-83ab-c269f4ef4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolationforrest_uni(df): #à reprendre pour la partie final score\n",
    "    #\n",
    "    # theta between 0 and 1 to decide a global outlier\n",
    "    #\n",
    "    #encode categorical into multiple code\n",
    "    #df=multicolumnlabelencoder(df)\n",
    "    \n",
    "    #obj_df = df.select_dtypes(include=['object','category','datetime64','bool']).copy()\n",
    "    #df=class_all.MultiColumnLabelEncoder(columns = obj_df.columns).fit_transform(df)\n",
    "    #df=OneHotEncoder(columns=obj_df.columns).fit_transform(df)\n",
    "\n",
    "    #score of isolation forrest\n",
    "    iforest=pd.DataFrame()\n",
    "    for columns in df.columns:\n",
    "        #print(columns)\n",
    "        model=IsolationForest(n_estimators=90, contamination=0.2)\n",
    "        model.fit(df[[columns]])\n",
    "        iforest['anomaly_'+columns ]=model.predict(df[[columns]])\n",
    "        \n",
    "    iforest=iforest.replace(1,0)\n",
    "    iforest=iforest.replace(-1, 1)\n",
    "\n",
    "    return(iforest) #return the matrix for 0 no outliers attributes and 1 outlier attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81107a55-148e-4665-8fd2-874ea4829c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://betterprogramming.pub/anomaly-detection-with-isolation-forest-e41f1f55cc6\n",
    "\n",
    "def isolationforrest_multi(df): #, threshold):  \n",
    "    #\n",
    "    # theta between 0 and 1 to decide a global outlier\n",
    "    #\n",
    "    #encode categorical into multiple code\n",
    "    #obj_df = df.select_dtypes(include=['object','category','datetime64','bool']).copy()\n",
    "    #df=multicolumnlabelencoder(df)\n",
    "    \n",
    "    #ALTERNATIVE the results are awfull\n",
    "   # iforest=pd.DataFrame()\n",
    "   # for columns in df.columns:\n",
    "        #print(columns)\n",
    "       # model=IsolationForest(n_estimators=50, max_samples='auto', contamination='auto',max_features=1.0)\n",
    "      #  model.fit(df[[columns]])\n",
    "      #  iforest['anomaly_'+columns ]=model.predict(df[[columns]])\n",
    "        \n",
    "    #iforest['Final_score']='normal' #normal (encode 1 for normal and -1 for outliers)\n",
    "   # for obs in range(iforest.shape[0]):\n",
    "      #  score_obs=0\n",
    "       # for columns in df.columns:\n",
    "         #   if iforest.loc[obs,'anomaly_'+columns ]==-1:\n",
    "           #     score_obs=score_obs+1\n",
    "      #  if score_obs/df.shape[1] > threshold: #% de variables déclarées outliers\n",
    "        #    iforest.loc[obs,'Final_score']='outliers'\n",
    "        #print(score_obs/df.shape[1])\n",
    "   # df['label_outliers']= 1\n",
    "   # df.loc[np.array(iforest['Final_score'].values)=='outliers', 'label_outliers']=-1\n",
    "\n",
    "    \n",
    "    \n",
    "    iforest=IsolationForest(n_estimators=90, contamination=0.2)\n",
    "    pred= iforest.fit_predict(df)\n",
    "    df['scores']=iforest.decision_function(df)\n",
    "    df['anomaly_label']=pred\n",
    "    #in this case iforest_df['anomaly_label']\n",
    "   \n",
    "    return df #, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3985ce9-654f-46c5-bba2-edb60c718baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(known_outliers,predict_outliers):\n",
    "    acc=accuracy_score(known_outliers, predict_outliers)\n",
    "    #fpr, tpr, thresholds = metrics.roc_curve(known_outliers, predict_outliers, pos_label=2)\n",
    "    #auc=metrics.auc(fpr, tpr)\n",
    "    f1=f1_score(known_outliers, predict_outliers, average=None)\n",
    "    \n",
    "\n",
    "    return acc,f1   #auc if needed to be added but problem in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbbb37-983a-4209-9da3-44ca3d5af057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def LOF_b(df,n_neighbors,contamination):    \n",
    "    \n",
    "    #encode categorical into multiple code\n",
    "    #df=multicolumnlabelencoder(df)\n",
    "    #obj_df = df.select_dtypes(include=['object']).copy()\n",
    "    #df=MultiColumnLabelEncoder(columns = obj_df.columns).fit_transform(df)\n",
    "\n",
    "    # fit the model for outlier detection (default)\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "    y_pred=lof.fit_predict(df)\n",
    "\n",
    "    lof_scores = lof.negative_outlier_factor_ #outlier scores without contamination value\n",
    "    thresh = np.quantile(lof_scores, .03) #definotion of the threshold\n",
    "\n",
    "\n",
    "    detected_outliers =  np.array([1] *df.shape[0])\n",
    "    detected_outliers[lof_scores<=thresh]=-1\n",
    "    \n",
    "    return(y_pred,detected_outliers, thresh) #y_pred with fixed contamination, detected_out with a made thresh\n",
    "\n",
    "\n",
    "    #possibility to visualize it with pca like the IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ac3b7-cbb0-40a7-bf67-bf3ed926eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_method(df):\n",
    "    \n",
    "    #encode categorical into multiple code if needed\n",
    "    #df=multicolumnlabelencoder(df)\n",
    "    #obj_df = df.select_dtypes(include=['object']).copy()\n",
    "    #df=MultiColumnLabelEncoder(columns = obj_df.columns).fit_transform(df)\n",
    "    \n",
    "    #compute the distance thanks to KNN  to find eps\n",
    "    neighbors = NearestNeighbors(n_neighbors=math.floor(math.sqrt(df.shape[0])))\n",
    "    neighbors_fit = neighbors.fit(df)\n",
    "    distances, indices = neighbors_fit.kneighbors(df)\n",
    "\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    distancesbis=[0 for x in range(len(distances))]\n",
    "    for i in range(len(distancesbis)-1):\n",
    "        #print(i)\n",
    "        distancesbis[i] = distances[i+1]\n",
    "        i=i+1\n",
    "    diff=distancesbis-distances\n",
    "\n",
    "    thedistance=distances[np.where(diff>round(np.percentile(diff, 75)))] #hyper parameters\n",
    "    try:\n",
    "        eps_knn=thedistance[0]\n",
    "    except IndexError:\n",
    "        eps_knn=0.5\n",
    "    \n",
    "\n",
    "    # specify & fit model\n",
    "    model = DBSCAN(eps = eps_knn, min_samples=50, metric='manhattan').fit(df) #parameters found with optuna\n",
    "    # outliers dataframe\n",
    "    outliers = df[model.labels_ == -1]\n",
    "    y_pred=model.labels_ \n",
    "    y_pred[y_pred!=-1]=1 #because can have several classes, -1 is noise and could be others classes\n",
    "    \n",
    "    return(outliers,y_pred,eps_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635ef0cf-2e92-4def-952f-a4828eabf09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_threshold_method(scores, margin):\n",
    "    q1 = np.percentile(scores, 25, interpolation='midpoint')\n",
    "    q3 = np.percentile(scores, 75, interpolation='midpoint')\n",
    "    iqr = q3-q1\n",
    "    lower_range = q1 - (1.5 * iqr)\n",
    "    upper_range = q3 + (1.5 * iqr)\n",
    "    lower_range = lower_range - margin\n",
    "    upper_range = upper_range + margin\n",
    "    return lower_range, upper_range\n",
    "\n",
    "#cf description de la function dans outliers_research\n",
    "\n",
    "def OdinOutlier(data,n_neighbors):\n",
    "    margin=0\n",
    "    radius=1.0\n",
    "    algorithm='auto'\n",
    "    leaf_size=30\n",
    "    metric='minkowski'\n",
    "    p=2\n",
    "    metric_params=None\n",
    "    n_jobs=None\n",
    "\n",
    "    nn = neighbors.NearestNeighbors(n_neighbors=n_neighbors, radius=radius, algorithm=algorithm, leaf_size=leaf_size, metric=metric, p=p, metric_params=metric_params, n_jobs=n_jobs)\n",
    "    #data=multicolumnlabelencoder(data)\n",
    "    nn.fit(data)\n",
    "    graph = nn.kneighbors_graph()\n",
    "    indegree = graph.sum(axis=0)  # sparse matrix\n",
    "\n",
    "    # smaller indegree means more of an anomaly\n",
    "    # simple conversion to [0,1] so larger score is more of anomaly\n",
    "    scores = (indegree.max() - indegree) / indegree.max()\n",
    "    scores = np.array(scores)[0]\n",
    "\n",
    "    lower_range, upper_range = iqr_threshold_method(scores, margin)\n",
    "\n",
    "    anomaly_index = []\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i] < lower_range or scores[i] > upper_range:\n",
    "            scores[i]=-1\n",
    "            anomaly_index.append(i)\n",
    "        else:\n",
    "            scores[i]=1\n",
    "\n",
    "    return anomaly_index, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050f29c-b5d5-4638-a234-9bac1dbc6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manu_uni_zscore(df, threshold,majority_perc, rare_perc): #    threshold=3, majority=0.95, rare=0.1\n",
    "\n",
    "    import numpy as npa\n",
    "    from scipy import stats\n",
    "    from scipy.stats import zscore\n",
    "\n",
    "    matrix_out=np.zeros([df.shape[0],df.shape[1]])\n",
    "    detected_outliers=np.array([1] *df.shape[0])\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns #'int','float', 'cfloat'\n",
    "\n",
    "\n",
    "    #first option for numeric: z score\n",
    "    #print(numeric_cols)\n",
    "    if len(numeric_cols)>0: \n",
    "        data_zscore =df[numeric_cols].apply(stats.zscore)\n",
    "        data_zscore = data_zscore.abs()\n",
    "\n",
    "    #outliers preprocessing numeric\n",
    "    for var in data_zscore.columns: #name of the var, here only numeric\n",
    "        var_out=0\n",
    "        for i in range(data_zscore.shape[0]):\n",
    "        #print(i)\n",
    "            if data_zscore.loc[i,var] > threshold:\n",
    "                matrix_out[i, list(df.columns).index(var)]=1\n",
    "                detected_outliers[i]=-1\n",
    "\n",
    "\n",
    "\n",
    "    data_cat = df.select_dtypes(include=['object']).copy() #, \"category\n",
    "\n",
    "    #outliers preprocessing categoric\n",
    "    for var in data_cat.columns: #name of the var, here only cat\n",
    "\n",
    "        dict_var= pd.DataFrame.from_dict(data_cat[var].value_counts()).reset_index()\n",
    "        dict_var=dict_var.rename(columns={'attribut': var, var:'count'})\n",
    "        dict_var['count']=dict_var['count'] #/df.shape[0]\n",
    "        #print(dict_var)\n",
    "\n",
    "        # observation rare\n",
    "        liste_rare=list(dict_var[(dict_var['count']==min(dict_var['count']))& (median(dict_var['count'])>=rare_perc*df.shape[0])]['index'])\n",
    "        if len(liste_rare)!=0:\n",
    "            where_errors=np.isin(df[var], liste_rare)\n",
    "\n",
    "            matrix_out[where_errors, list(df.columns).index(var)]=1\n",
    "\n",
    "        #ne correspond pas à la majorité \n",
    "        dict_count= pd.DataFrame.from_dict(dict_var['count'].value_counts()).reset_index()\n",
    "        #print(dict_count)\n",
    "        major=list(dict_count[dict_count['count']>majority_perc*df.shape[0]]['index'])\n",
    "        if len(major)!=0:\n",
    "            liste_rare=list(dict_var[dict_var['count']!=major[0]]['index'])\n",
    "            where_errors=np.isin(df[var], liste_rare)\n",
    "            matrix_out[where_errors, list(df.columns).index(var)]=1\n",
    "            detected_outliers[where_errors]=-1\n",
    "\n",
    "        \n",
    "    return(matrix_out, detected_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e1610-b2de-4e6e-8eb6-97b297bf4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manu_iqr(df,majority_perc, rare_perc):\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    from scipy.stats import zscore\n",
    "    import seaborn as sns\n",
    "    import statistics\n",
    "\n",
    "    matrix_out=np.zeros([df.shape[0],df.shape[1]])\n",
    "    detected_outliers=np.array([1] *df.shape[0])\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns #'int','float', 'cfloat'\n",
    "\n",
    "\n",
    "    #first option for numeric:IQR for outliers detection\n",
    "    #print(numeric_cols)\n",
    "    if len(numeric_cols)>0: \n",
    "        for var in numeric_cols: #name of the var, here only numeric\n",
    "            data=df[var]\n",
    "            sort_data = np.sort(df[var])\n",
    "            Q1 = np.percentile(data, 25, interpolation = 'midpoint') \n",
    "            Q2 = np.percentile(data, 50, interpolation = 'midpoint') \n",
    "            Q3 = np.percentile(data, 75, interpolation = 'midpoint')\n",
    "            IQR = Q3 - Q1 \n",
    "            low_lim = Q1 - 1.5 * IQR\n",
    "            up_lim = Q3 + 1.5 * IQR\n",
    "\n",
    "            for i in range(len(data)):\n",
    "                if ((data[i]> up_lim) or (data[i]<low_lim)):\n",
    "                    matrix_out[i, list(df.columns).index(var)]=1\n",
    "                    detected_outliers[i]=-1\n",
    "                    \n",
    "\n",
    "\n",
    "    data_cat = df.select_dtypes(include=['object']).copy() #, \"category\n",
    "\n",
    "    #outliers preprocessing categoric\n",
    "    for var in data_cat.columns: #name of the var, here only cat\n",
    "\n",
    "        dict_var= pd.DataFrame.from_dict(data_cat[var].value_counts()).reset_index()\n",
    "        dict_var=dict_var.rename(columns={'attribut': var, var:'count'})\n",
    "        dict_var['count']=dict_var['count'] #/df.shape[0]\n",
    "        #print(dict_var)\n",
    "\n",
    "        # observation rare c'est le minimum et la medianne a plus de 10% des observations (plusieurs catégories balanced)\n",
    "        liste_rare=list(dict_var[(dict_var['count']==min(dict_var['count']))& (median(dict_var['count'])>=rare_perc*df.shape[0])]['index'])\n",
    "        if len(liste_rare)!=0:\n",
    "            where_errors=np.isin(df[var], liste_rare)\n",
    "\n",
    "            matrix_out[where_errors, list(df.columns).index(var)]=1\n",
    "\n",
    "        #ne correspond pas à la majorité qui a plus de 0.95 des données\n",
    "        dict_count= pd.DataFrame.from_dict(dict_var['count'].value_counts()).reset_index()\n",
    "        #print(dict_count)\n",
    "        major=list(dict_count[dict_count['count']>majority_perc*df.shape[0]]['index'])\n",
    "        if len(major)!=0:\n",
    "            liste_rare=list(dict_var[dict_var['count']!=major[0]]['index'])\n",
    "            where_errors=np.isin(df[var], liste_rare)\n",
    "            matrix_out[where_errors, list(df.columns).index(var)]=1\n",
    "            detected_outliers[where_errors]=-1\n",
    "\n",
    "    return(matrix_out, detected_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c142214-085a-48b6-9890-0d5fa9844ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_zscore(df,threshold): #    threshold=3\n",
    "\n",
    "    import numpy as npa\n",
    "    from scipy import stats\n",
    "    from scipy.stats import zscore\n",
    "\n",
    "\n",
    "    detected_outliers=np.array([1] *df.shape[0])\n",
    "\n",
    "    numeric_cols = df.columns #'int','float', 'cfloat'\n",
    "\n",
    "\n",
    "    #first option for numeric: z score\n",
    "    #print(numeric_cols)\n",
    "    if len(numeric_cols)>0: \n",
    "        data_zscore =df[numeric_cols].apply(stats.zscore)\n",
    "        data_zscore = data_zscore.abs()\n",
    "\n",
    "    #outliers preprocessing numeric\n",
    "    for var in data_zscore.columns: #name of the var, here only numeric\n",
    "        var_out=0\n",
    "        for i in range(data_zscore.shape[0]):\n",
    "        #print(i)\n",
    "            if data_zscore.loc[i,var] > threshold:\n",
    "                detected_outliers[i]=-1\n",
    "\n",
    "\n",
    "        \n",
    "    return(detected_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99840b-6044-41ff-b01f-a310e96ff7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_iqr(df):\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    from scipy.stats import zscore\n",
    "    import seaborn as sns\n",
    "    import statistics\n",
    "\n",
    "    detected_outliers=np.array([1] *df.shape[0])\n",
    "    numeric_cols = df.columns #'int','float', 'cfloat'\n",
    "\n",
    "\n",
    "    #first option for numeric:IQR for outliers detection\n",
    "    #print(numeric_cols)\n",
    "    if len(numeric_cols)>0: \n",
    "        for var in numeric_cols: #name of the var, here only numeric\n",
    "            data=df[var]\n",
    "            sort_data = np.sort(df[var])\n",
    "            Q1 = np.percentile(data, 25, interpolation = 'midpoint') \n",
    "            Q2 = np.percentile(data, 50, interpolation = 'midpoint') \n",
    "            Q3 = np.percentile(data, 75, interpolation = 'midpoint')\n",
    "            IQR = Q3 - Q1 \n",
    "            low_lim = Q1 - 1.5 * IQR\n",
    "            up_lim = Q3 + 1.5 * IQR\n",
    "\n",
    "            for i in range(len(data)):\n",
    "                if ((data[i]> up_lim) or (data[i]<low_lim)):\n",
    "                      detected_outliers[i]=-1\n",
    "\n",
    "\n",
    "    return(detected_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ecce0-9ab1-4871-aa1a-9f23c91c89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_nainf(df): #pd dataframe\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True) #replace inf by nan\n",
    "    \n",
    "    #the nan are replaced by a simple univariate feature imputation\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imp = SimpleImputer(strategy=\"most_frequent\") #easier with the paramters\n",
    "    #most frequent so it can work for both numeric and categoric without affecting the fact of being outliers. \n",
    "    \n",
    " \n",
    "    return imp.fit_transform(df)\n",
    "\n",
    "def support(pattern,liste_patterns):#frequency of an occurring pattern d\n",
    "    #print(d_i,pattern)\n",
    "    \n",
    "    #n is the number of records in the dataset\n",
    "    #d in D: categorical pattern where attributes occur only once\n",
    "    #xi(D): cetegorical values of point xi for attributes D\n",
    "    \n",
    "    support=0 #initialisation\n",
    "    \n",
    "    d=len(pattern)\n",
    "    #print(d)\n",
    "    for liste_pattern in liste_patterns: #size the depth of the tree\n",
    "        if pattern==liste_pattern[:d]:\n",
    "            support=support+1\n",
    "    \n",
    "    if support==0:\n",
    "        print('error support=0, pattern not in the dataframe')\n",
    "        support=1\n",
    "    \n",
    "    return support #support of the pattern_Xi\n",
    "\n",
    "def lda_function(df): #least discriminative attribute\n",
    "    L_a=[]\n",
    "    new_order=[]\n",
    "    for variable in df.columns:\n",
    "        L_a.append(df[variable].nunique())\n",
    "    L_a=sorted(L_a, reverse=True)\n",
    "\n",
    "    while L_a[0]==df.shape[0]:\n",
    "        L_a.append(L_a.pop(0))\n",
    "\n",
    "    for nbr_unique in L_a:\n",
    "        for var in df.columns:\n",
    "            if (df[var].nunique()==nbr_unique):\n",
    "                new_order.append(var)\n",
    "    new_order\n",
    "\n",
    "    return new_order\n",
    "def LDA_tree(df,new_order):\n",
    "    #print(new_order)\n",
    "    \n",
    "    df=df[new_order]\n",
    "    \n",
    "    liste_patterns=df.values.tolist() #each observation is converted in a liste\n",
    "    \n",
    "    return liste_patterns\n",
    "\n",
    "def score1(pattern: list,liste_patterns:list, threshold): #pattern of x_i\n",
    "    score_out=0\n",
    "    d=len(pattern)\n",
    "    if abs(d)>0:\n",
    "        for d_i in range(1,d+1):\n",
    "            #print('d_i',d_i)\n",
    "            if support(pattern[:d_i],liste_patterns) > threshold:\n",
    "                score_out=score_out+ (1/(support(pattern[:d_i],liste_patterns)*abs(d_i)))\n",
    "        #abs(d) is the depth of the tree\n",
    "        #d represents the minimum infrequent pattern of the LDA-tree\n",
    "    else: \n",
    "        print('Erreur d negative')\n",
    "    return score_out #if close to 1, xi high proba of being outlier\n",
    "#algo 1: attribute bagging algorithm\n",
    "#randomly create pultiple subsets of the data to handle irrevelant and redundant attributes\n",
    "\n",
    "from random import sample \n",
    "\n",
    "def attribute_bagging(df,T):\n",
    "    #D_t attribute subset \n",
    "    #b = maximum nbr of attributes in attribute bag \n",
    "    subsets=None\n",
    "    for t in (1,T): #T iteration \n",
    "        #take a random sample of the attributes without replacement \n",
    "        selected_attributes=df.sample(df.shape[1]) \n",
    "        b=max(selected_attributes.shape[1])\n",
    "        #d in b attributes in the t th iteration\n",
    "        subsets= selected_attributes #add selected attributes of D\n",
    "        \n",
    "    return subsets\n",
    "\n",
    "#algo 2 postponed outlier detection in the paper, adapted\n",
    "\n",
    "def postponed_outlier_detection(df,sigma): #for categorical attributes\n",
    "    #input D dataset and sigma percentage (threshold)\n",
    "    \n",
    "    obj_df = df.select_dtypes(include=['object']).copy()\n",
    "    df_cat=obj_df\n",
    "    liste_var_cat= list(obj_df.columns)\n",
    "    #print(liste_var_cat)\n",
    "    m_c=len(liste_var_cat) #nbr of catagorical attributes\n",
    "    if m_c!=0: #there are categorical variables\n",
    "        threshold=sigma*df.shape[0]\n",
    "\n",
    "\n",
    "        X_score1=[]\n",
    "        X_freq=[] #vector containing all records\n",
    "        X_i=[]\n",
    "        X_infrequent=[]\n",
    "\n",
    "\n",
    "        new_order=lda_function(df_cat)\n",
    "        #print(lda)\n",
    "        #print(variable_first)\n",
    "        liste_patterns= LDA_tree(df_cat,new_order)\n",
    "        #print(liste_patterns)\n",
    "        for i in range(df_cat.shape[0]):\n",
    "            pattern=liste_patterns[i]\n",
    "            #support_i=support(d_i,pattern,liste_patterns)\n",
    "            X_freq.append(pattern)\n",
    "            X_i.append(i)\n",
    "            #print(pattern)\n",
    "            #print(type(pattern))\n",
    "            #print(liste_patterns)\n",
    "            score_out=score1(pattern,liste_patterns, threshold)\n",
    "            support_i=support(pattern,liste_patterns)\n",
    "            #print(score_out)\n",
    "            X_score1.append(score_out) #to be reshape in 2 dimension i and score, and to be ranked in a decreasing order\n",
    "            if support_i <=threshold:\n",
    "                X_infrequent.append(i)\n",
    "    else:\n",
    "        X_score1=[]\n",
    "        X_freq=[]\n",
    "        X_infrequent=[]\n",
    "        print('No categorical variables')\n",
    "\n",
    "        \n",
    "    return(X_score1,X_freq,X_infrequent)\n",
    "\n",
    "def score2(df):\n",
    "    import pyclustering\n",
    "    from pyclustering.cluster.kmedians import kmedians\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    N=df.shape[0]\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    df_num=df.select_dtypes(include=numerics)\n",
    "    liste_var_num=list(df_num.columns) #list of numerical variables\n",
    "    \n",
    "    sigma=0.05 #TO FIX\n",
    "    threshold=int(sigma*N)\n",
    "    \n",
    "    if len(liste_var_num)!=0: #si il y a des variables numériques\n",
    "\n",
    "        range_n_clusters = [2, 3, 4, 5, 6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "        silouhette_list=[]\n",
    "\n",
    "\n",
    "        #define M\n",
    "        for n_clusters in range_n_clusters:\n",
    "\n",
    "            initial_medians=df_num.sample(n=n_clusters)\n",
    "\n",
    "            # Initialize the clusterer with n_clusters value and a random generator\n",
    "            # seed of 10 for reproducibility.\n",
    "            kmedians_instance = kmedians(df_num, initial_medians)\n",
    "            kmedians_instance.process()\n",
    "            cluster_labels = kmedians_instance.get_clusters()\n",
    "            medians = kmedians_instance.get_medians()\n",
    "\n",
    "            labels=np.zeros(df_num.shape[0])\n",
    "            for i in range(len(cluster_labels)):\n",
    "                labels[cluster_labels[i]]=i+1\n",
    "            #print(labels)\n",
    "\n",
    "            # The silhouette_score gives the average value for all the samples.\n",
    "            # This gives a perspective into the density and separation of the formed\n",
    "            # clusters\n",
    "            silhouette_avg = silhouette_score(df_num, labels)\n",
    "            silouhette_list.append( silhouette_avg )\n",
    "            #print(\"For n_clusters =\", n_clusters,\n",
    "                 # \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "            minimum=np.min(silouhette_list)\n",
    "            #print(minimum)\n",
    "            M=np.where(silouhette_list==minimum)[0][0]+2 #decalage python + 1 et + 1 de cluster pas commençant à 1\n",
    "\n",
    "        #print('The selected M is', M,'.' ) #M=3 #(nbr of classes we want + outliers)\n",
    "\n",
    "        #Kmedians most rebust than kmeans\n",
    "\n",
    "\n",
    "        #initialization with k points (k number choosen as the nbr of clusters ) and\n",
    "        #with d cordoniate: the number of numeric variables\n",
    "        initial_medians=[[0 for col in range(len(liste_var_num))] for row in range(M)] #fixed them empty this is easier\n",
    "        kmedians_instance = kmedians(df_num, initial_medians) #included the number of clusters\n",
    "        kmedians_instance.process()\n",
    "        cluster_labels = kmedians_instance.get_clusters()\n",
    "        medians = kmedians_instance.get_medians() #Medians are calculated instead of centroids.\n",
    "        #The algorithm is less sensitive to outliers than K-Means.\n",
    "\n",
    "        #score 2\n",
    "\n",
    "        X_score2=[]\n",
    "        for i in range(df_num.shape[0]):\n",
    "            score2=0\n",
    "            m_q= df_num.shape[1] #number of continuous attributes\n",
    "            for j in range(abs(m_q)):\n",
    "                score2=score2+abs(df_num.iloc[i,j] -medians[0][j]) #manhattan distance\n",
    "            X_score2.append(score2)\n",
    "\n",
    "        #if score2 increases, the closest centroid for record x_i is distant\n",
    "    else: #no variables numeriques\n",
    "        M=None\n",
    "        X_score2=[]\n",
    "    \n",
    "    return(M,X_score2)\n",
    "\n",
    "def final_score(df,M,X_score1,X_freq,X_infrequent,X_score2):\n",
    "    N=df.shape[0] #nbr d'observation x_i\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    obj_df = df.select_dtypes(include=['object']).copy()\n",
    "    liste_var_cat= obj_df.columns #liste des variables categoriques\n",
    "    num_df=df.select_dtypes(include=numerics)\n",
    "    liste_var_num=num_df.columns #list of numerical variables\n",
    "\n",
    "    score_rank=pd.DataFrame(columns=['df_index','score_cat','rank_cat','score_num','rank_num','final_rank'])\n",
    "\n",
    "    for i in range(N):\n",
    "        #compute for each attribute the scores\n",
    "        score_rank.loc[i,'df_index']=i\n",
    "        if len(X_score1)!=0: #not empty\n",
    "            score_rank.loc[i,'score_cat']=X_score1[i]\n",
    "        if len(X_score2)!=0: #not empty\n",
    "            score_rank.loc[i,'score_num']=X_score2[i]\n",
    "    score_meltzer=score_rank\n",
    "\n",
    "\n",
    "    #sort the individues by their scores for each attributes    \n",
    "    rank_cat=score_rank.sort_values('score_cat')\n",
    "    rank_cat=rank_cat.reset_index()\n",
    "    rank_num=score_rank.sort_values('score_num')\n",
    "    #print(rank_num)\n",
    "    rank_num=rank_num.reset_index()\n",
    "    #print(rank_num)\n",
    "\n",
    "    for i in range(N):\n",
    "        #print(rank_cat[rank_cat['df_index']==i])\n",
    "        score_meltzer.loc[i,'rank_cat']=rank_cat[rank_cat['df_index']==i].index[0]\n",
    "        score_meltzer.loc[i,'rank_num']=rank_num[rank_num['df_index']==i].index[0]\n",
    "        #add the rank for each attribute into the dataset\n",
    "\n",
    "    score_meltzer['final_rank']=(score_meltzer['rank_cat']+score_meltzer['rank_num'])/2\n",
    "\n",
    "\n",
    "    return(score_meltzer) #return the dataset to have all the scores and check them if needed but decision on final_rank\n",
    "        \n",
    "\n",
    "def meltzer_algo(df,sigma): #sigma=0.05\n",
    "    import signal\n",
    "    X_score1,X_freq,X_infrequent=postponed_outlier_detection(df,sigma)\n",
    "    M,X_score2=score2(df)\n",
    "    score_meltzer=final_score(df,M,X_score1,X_freq,X_infrequent,X_score2)\n",
    "    score_meltzer=score_meltzer.sort_values('final_rank')\n",
    "    score_meltzer= score_meltzer.reset_index()\n",
    "\n",
    "    thres=int(sigma*df.shape[0])\n",
    "    #print(thres)\n",
    "\n",
    "    outliers_metzler=np.array([1] *df.shape[0])\n",
    "    for i in range(len(score_meltzer.loc[:thres,'df_index'].values)):\n",
    "        outliers_metzler[score_meltzer.loc[:thres,'df_index'].values[i]]=-1\n",
    "    return outliers_metzler,M\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f0fb4-979b-432e-b778-8f88d8f07736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorielle(n):\n",
    "    n=int(n)\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        F = 1\n",
    "        \n",
    "        for k in range(2,n+1):\n",
    "            F = F * int(k)\n",
    "        return F\n",
    "    \n",
    "def fct_toknow_the_type(df,var):\n",
    "    nbr=df[df.columns[var]].nunique()\n",
    "    \n",
    "    if nbr==df.shape[0]:\n",
    "        return 'id'\n",
    "        \n",
    "    if nbr==2:\n",
    "        return 'binary'\n",
    "    from pandas.api.types import is_string_dtype\n",
    "    \n",
    "    if is_string_dtype(df[df.columns[var]])==True:\n",
    "        return 'nominal'\n",
    "    \n",
    "    from scipy.stats import shapiro\n",
    "    #drawback of Shapiro-Wilk test is,\n",
    "    #it is not reliable once the sample size (or) the length of a variable crosses 5,000.\n",
    "    \n",
    "    if shapiro(df[df.columns[var]])[1]>0.05:\n",
    "        \n",
    "        return 'gaussian'\n",
    "\n",
    "    else:\n",
    "        return 'count'\n",
    "    \n",
    "\n",
    "\n",
    "def Mixed_RBM_Simplified(df, a, Beta): #a=[1]*df.shape[0], Beta=0.05\n",
    "    \n",
    "    matrix_nrj=np.zeros(df.shape)\n",
    "\n",
    "    for var in range(1,df.shape[1]):\n",
    "        #print(var)\n",
    "            #energy fct\n",
    "        if fct_toknow_the_type(df,var)=='id':\n",
    "            #print('id')\n",
    "            matrix_nrj[:][var]= 1 #to have exp of 1 equals to zero\n",
    "\n",
    "        if fct_toknow_the_type(df,var)=='binary':\n",
    "            #print('binary')\n",
    "            for obs in range(df.shape[0]):\n",
    "                if obs==0:\n",
    "                    value=df.iloc[obs,var]\n",
    "                if df.iloc[obs,var]==value:\n",
    "                    df.iloc[obs,var]=0\n",
    "                else:\n",
    "                    df.iloc[obs,var]=1\n",
    "                #print(obs)\n",
    "                #print(df.iloc[obs,var], 'binary')\n",
    "                matrix_nrj[obs][var]= - a[obs]*df.iloc[obs,var]\n",
    "\n",
    "        if fct_toknow_the_type(df,var)=='gaussian':\n",
    "            #print('gaussian')\n",
    "            for obs in range(df.shape[0]):\n",
    "                #print(df.iloc[obs,var], 'gaussian')\n",
    "                matrix_nrj[obs][var]=0.5*(df.iloc[obs,var]**2) - a[obs]*df.iloc[obs,var]\n",
    "\n",
    "        if fct_toknow_the_type(df,var)=='nominal':#print('nominal')\n",
    "            for obs in range(df.shape[0]):\n",
    "                matrix_nrj[obs][var]=list(df[df.columns[var]]).count(df.iloc[obs,var])\n",
    "\n",
    "        if fct_toknow_the_type(df,var)=='count':\n",
    "            #print('count')\n",
    "            for obs in range(df.shape[0]):\n",
    "                #print(obs)\n",
    "                if df.iloc[obs,var]>10:\n",
    "                    fact=float(10000) #sinon trop gros\n",
    "                else:\n",
    "                    fact=float(factorielle(df.iloc[obs,var])) #limite du factoriel for acceptance\n",
    "                matrix_nrj[obs][var]= np.log(fact) - a[obs]*df.iloc[obs,var]\n",
    "\n",
    "        outliers=np.array([1]*df.shape[0])\n",
    "        score_nrj_list=[]\n",
    "        for nb_obs in range(df.shape[0]):\n",
    "            score_nrj=-np.log(abs(np.sum(matrix_nrj[nb_obs])))\n",
    "            score_nrj_list.append(score_nrj)\n",
    "            #print(score_nrj)\n",
    "        sort_nrj_list=sorted(score_nrj_list)\n",
    "        Beta_values=[]\n",
    "\n",
    "        for nbr in range(int(Beta*df.shape[0])):\n",
    "            #print(nbr)\n",
    "            Beta_values.append(sort_nrj_list[nbr])\n",
    "            Beta_values.append(sort_nrj_list[-nbr] )\n",
    "\n",
    "\n",
    "        for nbr_obs in range(df.shape[0]):\n",
    "\n",
    "            if (score_nrj_list[nbr_obs] in Beta_values):\n",
    "                outliers[nbr_obs]=-1\n",
    "    return(outliers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3ab21-2b60-4368-af35-acdd5c833df5",
   "metadata": {},
   "source": [
    "### How to call the functions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e477245-7e40-4b64-9364-e5e12ad1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import your dataset\n",
    "df=pd.read_csv(path + name + '.csv', sep=',') #df with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6803b3-deb6-4f97-88a4-a94a2c071efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi label encoding if needed(mixed and categorical values)\n",
    "obj_df = df.select_dtypes(include=['object', 'bool']).copy()\n",
    "df=MultiColumnLabelEncoder(columns = obj_df.columns).fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0ad5a-f19d-4af6-9240-0ca80fcad12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolation forest multivariate detection, need encoding\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "df_if=isolationforrest_multi(df) #,  threshold)\n",
    "#acc=accuracy_score(know_outliers,np.array(df_if['anomaly_label'].values))\n",
    "#f1=f1_score(know_outliers,np.array(df_if['anomaly_label'].values), average=None) #df_if['label_outliers']\n",
    "#acc, f1=metrics(know_outliers,np.array(df_if['label_outliers'].values)) #df_if['label_outliers'])) #auc and f1 score\n",
    "#matrix= confusion_matrix(know_outliers,np.array(df_if['anomaly_label'].values))#df_if['label_outliers'])\n",
    "outliers_vector=df_if['anomaly_label']\n",
    "(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae62f9-04a7-44be-9361-2d1c8c1a6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lof by Beunig detection, need encoding\n",
    "\n",
    "n_neighbors=28\n",
    "contamination=0.15\n",
    "detected_outliers,thesh=LOF_b(df,n_neighbors,contamination)\n",
    "acc=accuracy_score(know_outliers,detected_outliers)\n",
    "\n",
    "\n",
    "#DBSCAN, need encoding\n",
    "\n",
    "\n",
    "outliers,found_labels,eps_knn=dbscan_method(df)\n",
    "found_labels[found_labels!=-1]=1\n",
    "detected_outliers=found_labels\n",
    "\n",
    "#odin, need encoding\n",
    "n_neighbors=28\n",
    "anomaly_index,detected_outliers= OdinOutlier(np.array(df),n_neighbors)\n",
    "\n",
    "#Zscore need encoding\n",
    "threshold=3\n",
    "detected_outliers= multi_zscore(df,threshold)\n",
    "\n",
    "#IQR need encoding or just numeric\n",
    "detected_outliers=multi_iqr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2eb23-e347-4408-aadd-b743d88dea53",
   "metadata": {},
   "source": [
    "\n",
    "Without encoding the categorical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8925873-f502-46c2-8ee7-ca4b7b3af933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALGO: Meltzer-------------------------------------------------------------------------------------------------\n",
    "sigma=0.05\n",
    "\n",
    "detected_outliers,M=meltzer_algo(df,sigma)\n",
    "\n",
    "#ALGO: RBM_simplied-------------------------------------------------------------------------------------------------\n",
    "Beta=0.05\n",
    "a=[1]*df.shape[0]\n",
    "detected_outliers=Mixed_RBM_Simplified(df, a, Beta)\n",
    "\n",
    "#matrix for univariate and vector for multivariate\n",
    "\n",
    "#ALGO: Zscore + ICL-------------------------------------------------------------------------------------------------\n",
    "threshold=2\n",
    "majority_perc=0.95\n",
    "rare_perc=0.19\n",
    "matrix_out,detected_outliers=manu_uni_zscore(df,threshold,majority_perc,rare_perc)\n",
    "\n",
    "#ALGO: IQR+ ICL-------------------------------------------------------------------------------------------------\n",
    "majority_perc=0.95\n",
    "rare_perc=0.19\n",
    "matrix_out,detected_outliers=manu_iqr(df,majority_perc,rare_perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728154b0-72ab-409e-8238-f3c4a7e937d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for univariate\n",
    "\n",
    "#isolation forests\n",
    "iforest_matrix=isolationforrest_uni(df_encode)\n",
    "TP, FN, FP,TN, acc, f1=confusion_matrix_for_matrix(matrix_out= matrix_out, algo_matrix=matrix_pred) #matrix True or false if founded\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
